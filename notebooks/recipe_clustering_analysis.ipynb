{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recipe Clustering Analysis\n",
    "\n",
    "This notebook demonstrates three clustering approaches for grouping recipes based on their sensorik (odor) profiles:\n",
    "\n",
    "1. **HDBSCAN** - Density-based clustering (allows outliers)\n",
    "2. **Agglomerative** - Hierarchical clustering (no outliers)\n",
    "3. **FAISS** - Vector database with k-means (similarity search)\n",
    "\n",
    "## Key Concept: Weighted Sensorik Importance\n",
    "\n",
    "Each recipe has up to 16 sensorik descriptors where:\n",
    "- **Sensorik_1** = Most identifiable odor (weight = 1.0)\n",
    "- **Sensorik_16** = Least identifiable odor (weight = 0.0625)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy scikit-learn hdbscan matplotlib faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "import hdbscan\n",
    "import faiss\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"All packages loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('../data/gold/Versuchsdaten.csv')\n",
    "\n",
    "print(f\"Total rows: {len(df)}\")\n",
    "print(f\"Columns: {len(df.columns)}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify key columns\n",
    "recipe_col = 'Rez.-Nr.'\n",
    "sensorik_cols = [f'Sensorik_{i}' for i in range(1, 17)]\n",
    "\n",
    "print(f\"Recipe column: {recipe_col}\")\n",
    "print(f\"Unique recipes: {df[recipe_col].nunique()}\")\n",
    "print(f\"\\nSensorik columns: {sensorik_cols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore sensorik values\n",
    "all_sensorik = []\n",
    "for col in sensorik_cols:\n",
    "    vals = df[col].dropna().unique()\n",
    "    all_sensorik.extend([str(v).lower().strip() for v in vals if pd.notna(v)])\n",
    "\n",
    "unique_sensorik = sorted(set(all_sensorik))\n",
    "print(f\"Unique sensorik descriptors: {len(unique_sensorik)}\")\n",
    "print(f\"\\nSample descriptors: {unique_sensorik[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction with Weighted Importance\n",
    "\n",
    "We create weighted feature vectors where:\n",
    "- Each dimension corresponds to a unique sensorik term\n",
    "- Weight decreases from Sensorik_1 (1.0) to Sensorik_16 (0.0625)\n",
    "\n",
    "$$\\text{weight}(\\text{position}) = \\frac{17 - \\text{position}}{16}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weight(position):\n",
    "    \"\"\"Calculate importance weight based on sensorik position.\"\"\"\n",
    "    return (17 - position) / 16\n",
    "\n",
    "# Visualize the weight distribution\n",
    "positions = range(1, 17)\n",
    "weights = [calculate_weight(p) for p in positions]\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.bar(positions, weights, color='steelblue', edgecolor='black')\n",
    "plt.xlabel('Sensorik Position')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Sensorik Importance Weights by Position')\n",
    "plt.xticks(positions)\n",
    "for i, w in enumerate(weights):\n",
    "    plt.text(i+1, w+0.02, f'{w:.3f}', ha='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_term(term):\n",
    "    \"\"\"Normalize a sensorik term.\"\"\"\n",
    "    if pd.isna(term) or not isinstance(term, str):\n",
    "        return None\n",
    "    term = term.lower().strip().replace('\"', '').replace(\"'\", \"\")\n",
    "    term = term.rstrip('.,;:')\n",
    "    return term if len(term) >= 2 else None\n",
    "\n",
    "def build_vocabulary(df, sensorik_cols):\n",
    "    \"\"\"Build vocabulary from all sensorik columns.\"\"\"\n",
    "    all_terms = set()\n",
    "    for col in sensorik_cols:\n",
    "        if col in df.columns:\n",
    "            terms = df[col].dropna().apply(normalize_term)\n",
    "            all_terms.update([t for t in terms if t])\n",
    "    vocabulary = sorted(all_terms)\n",
    "    vocab_to_idx = {term: idx for idx, term in enumerate(vocabulary)}\n",
    "    return vocabulary, vocab_to_idx\n",
    "\n",
    "def extract_recipe_vectors(df, recipe_col, sensorik_cols, vocabulary, vocab_to_idx):\n",
    "    \"\"\"Extract weighted feature vectors for each recipe.\"\"\"\n",
    "    recipes = df[recipe_col].unique().tolist()\n",
    "    n_recipes = len(recipes)\n",
    "    n_features = len(vocabulary)\n",
    "    \n",
    "    vectors = np.zeros((n_recipes, n_features))\n",
    "    \n",
    "    for recipe_idx, recipe in enumerate(recipes):\n",
    "        recipe_data = df[df[recipe_col] == recipe]\n",
    "        \n",
    "        for _, row in recipe_data.iterrows():\n",
    "            for position, col in enumerate(sensorik_cols, start=1):\n",
    "                if col in df.columns:\n",
    "                    term = normalize_term(row.get(col))\n",
    "                    if term and term in vocab_to_idx:\n",
    "                        weight = calculate_weight(position)\n",
    "                        vectors[recipe_idx, vocab_to_idx[term]] += weight\n",
    "    \n",
    "    # L2 normalize\n",
    "    vectors = normalize(vectors)\n",
    "    return vectors, recipes\n",
    "\n",
    "# Build vocabulary and extract vectors\n",
    "vocabulary, vocab_to_idx = build_vocabulary(df, sensorik_cols)\n",
    "recipe_vectors, recipes = extract_recipe_vectors(df, recipe_col, sensorik_cols, vocabulary, vocab_to_idx)\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"Number of recipes: {len(recipes)}\")\n",
    "print(f\"Vector shape: {recipe_vectors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cluster_names(cluster_labels, recipe_vectors, vocabulary, top_n=3):\n",
    "    \"\"\"Generate human-readable names for clusters based on distinctive terms.\"\"\"\n",
    "    cluster_names = {}\n",
    "    unique_labels = sorted(set(cluster_labels))\n",
    "    \n",
    "    # Calculate global average\n",
    "    global_centroid = recipe_vectors.mean(axis=0)\n",
    "    \n",
    "    for label in unique_labels:\n",
    "        if label == -1:\n",
    "            cluster_names[label] = \"Outliers\"\n",
    "            continue\n",
    "        \n",
    "        cluster_mask = cluster_labels == label\n",
    "        cluster_vectors = recipe_vectors[cluster_mask]\n",
    "        centroid = cluster_vectors.mean(axis=0)\n",
    "        \n",
    "        # Find distinctive terms\n",
    "        distinctiveness = centroid - global_centroid * 0.8\n",
    "        top_indices = np.argsort(distinctiveness)[-6:][::-1]\n",
    "        \n",
    "        distinctive_terms = []\n",
    "        for idx in top_indices:\n",
    "            if distinctiveness[idx] > 0 and centroid[idx] > 0.05:\n",
    "                distinctive_terms.append(vocabulary[idx].capitalize())\n",
    "            if len(distinctive_terms) >= top_n:\n",
    "                break\n",
    "        \n",
    "        if len(distinctive_terms) < 2:\n",
    "            top_indices = np.argsort(centroid)[-top_n:][::-1]\n",
    "            distinctive_terms = [vocabulary[i].capitalize() for i in top_indices]\n",
    "        \n",
    "        cluster_names[label] = \"-\".join(distinctive_terms[:top_n])\n",
    "    \n",
    "    return cluster_names\n",
    "\n",
    "def get_cluster_details(cluster_labels, recipe_vectors, recipes, vocabulary, cluster_names):\n",
    "    \"\"\"Get detailed information about each cluster.\"\"\"\n",
    "    details = {}\n",
    "    \n",
    "    for label in sorted(set(cluster_labels)):\n",
    "        cluster_mask = cluster_labels == label\n",
    "        cluster_recipes = [recipes[i] for i, m in enumerate(cluster_mask) if m]\n",
    "        cluster_vectors = recipe_vectors[cluster_mask]\n",
    "        \n",
    "        centroid = cluster_vectors.mean(axis=0)\n",
    "        top_indices = np.argsort(centroid)[-10:][::-1]\n",
    "        top_terms = [(vocabulary[i], centroid[i]) for i in top_indices]\n",
    "        \n",
    "        details[label] = {\n",
    "            'name': cluster_names.get(label, f\"Cluster {label}\"),\n",
    "            'recipes': cluster_recipes,\n",
    "            'centroid': centroid,\n",
    "            'top_terms': top_terms,\n",
    "            'size': len(cluster_recipes)\n",
    "        }\n",
    "    \n",
    "    return details\n",
    "\n",
    "def print_cluster_summary(details):\n",
    "    \"\"\"Print a summary of clusters.\"\"\"\n",
    "    for label in sorted(details.keys()):\n",
    "        info = details[label]\n",
    "        print(f\"\\n{'─' * 50}\")\n",
    "        print(f\"CLUSTER {label}: {info['name']}\")\n",
    "        print(f\"{'─' * 50}\")\n",
    "        print(f\"Recipes ({info['size']}): {', '.join(info['recipes'][:5])}{'...' if len(info['recipes']) > 5 else ''}\")\n",
    "        print(f\"Top terms:\")\n",
    "        for term, weight in info['top_terms'][:5]:\n",
    "            bar = '█' * int(weight * 40)\n",
    "            print(f\"  {term:15} {bar} ({weight:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(recipe_vectors, cluster_labels, cluster_names, recipes, title=\"Recipe Clusters\"):\n",
    "    \"\"\"Visualize clusters using t-SNE.\"\"\"\n",
    "    # Reduce dimensions with t-SNE\n",
    "    perplexity = min(5, len(recipes) - 1)\n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42, max_iter=1000)\n",
    "    coords = tsne.fit_transform(recipe_vectors)\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    \n",
    "    # Color palette\n",
    "    unique_labels = sorted(set(cluster_labels))\n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, max(len(unique_labels), 8)))\n",
    "    \n",
    "    # Plot 1: Scatter plot\n",
    "    ax1 = axes[0]\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        mask = cluster_labels == label\n",
    "        cluster_coords = coords[mask]\n",
    "        \n",
    "        if label == -1:\n",
    "            color = 'gray'\n",
    "            marker = 'x'\n",
    "            alpha = 0.5\n",
    "        else:\n",
    "            color = colors[i % len(colors)]\n",
    "            marker = 'o'\n",
    "            alpha = 0.8\n",
    "        \n",
    "        ax1.scatter(cluster_coords[:, 0], cluster_coords[:, 1],\n",
    "                   c=[color], marker=marker, s=150, alpha=alpha,\n",
    "                   label=cluster_names.get(label, f\"Cluster {label}\"),\n",
    "                   edgecolors='black', linewidths=0.5)\n",
    "    \n",
    "    # Add recipe labels\n",
    "    for i, recipe in enumerate(recipes):\n",
    "        ax1.annotate(recipe[:10], (coords[i, 0], coords[i, 1]),\n",
    "                    fontsize=7, alpha=0.7, ha='center', va='bottom')\n",
    "    \n",
    "    ax1.set_xlabel('t-SNE Dimension 1')\n",
    "    ax1.set_ylabel('t-SNE Dimension 2')\n",
    "    ax1.set_title(f'{title} - t-SNE Visualization')\n",
    "    ax1.legend(loc='upper left', fontsize=8)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Cluster sizes\n",
    "    ax2 = axes[1]\n",
    "    sizes = [list(cluster_labels).count(l) for l in unique_labels]\n",
    "    names = [cluster_names.get(l, f\"C{l}\")[:25] for l in unique_labels]\n",
    "    bar_colors = [colors[i % len(colors)] if l != -1 else 'gray' for i, l in enumerate(unique_labels)]\n",
    "    \n",
    "    bars = ax2.barh(range(len(unique_labels)), sizes, color=bar_colors)\n",
    "    ax2.set_yticks(range(len(unique_labels)))\n",
    "    ax2.set_yticklabels(names, fontsize=9)\n",
    "    ax2.set_xlabel('Number of Recipes')\n",
    "    ax2.set_title('Cluster Sizes')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    for bar, size in zip(bars, sizes):\n",
    "        ax2.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2,\n",
    "                str(size), va='center', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_radar(details, vocabulary, title=\"Cluster Sensorik Profiles\"):\n",
    "    \"\"\"Create radar chart for cluster profiles.\"\"\"\n",
    "    # Get top terms across all clusters\n",
    "    all_top_terms = set()\n",
    "    for label, info in details.items():\n",
    "        if label == -1:\n",
    "            continue\n",
    "        all_top_terms.update([t[0] for t in info['top_terms'][:8]])\n",
    "    \n",
    "    all_top_terms = sorted(all_top_terms)[:12]\n",
    "    n_terms = len(all_top_terms)\n",
    "    \n",
    "    if n_terms == 0:\n",
    "        print(\"No data for radar chart\")\n",
    "        return\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, n_terms, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "    \n",
    "    colors = plt.cm.Set2(np.linspace(0, 1, len(details)))\n",
    "    \n",
    "    for i, (label, info) in enumerate(details.items()):\n",
    "        if label == -1:\n",
    "            continue\n",
    "        \n",
    "        term_to_weight = {t[0]: t[1] for t in info['top_terms']}\n",
    "        values = [term_to_weight.get(term, 0) for term in all_top_terms]\n",
    "        values += values[:1]\n",
    "        \n",
    "        ax.plot(angles, values, 'o-', linewidth=2, label=info['name'],\n",
    "               color=colors[i % len(colors)])\n",
    "        ax.fill(angles, values, alpha=0.15, color=colors[i % len(colors)])\n",
    "    \n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(all_top_terms, fontsize=9)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0), fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Method 1: HDBSCAN Clustering\n",
    "\n",
    "HDBSCAN is a density-based clustering algorithm that:\n",
    "- Automatically determines the number of clusters\n",
    "- Identifies outliers (recipes that don't fit any cluster)\n",
    "- Does not force all points into clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run HDBSCAN clustering\n",
    "hdbscan_clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=2,\n",
    "    min_samples=1,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "hdbscan_labels = hdbscan_clusterer.fit_predict(recipe_vectors)\n",
    "\n",
    "n_clusters = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)\n",
    "n_noise = list(hdbscan_labels).count(-1)\n",
    "\n",
    "print(\"HDBSCAN Results:\")\n",
    "print(f\"  Clusters found: {n_clusters}\")\n",
    "print(f\"  Outliers: {n_noise} ({n_noise/len(recipes)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cluster names and details\n",
    "hdbscan_names = generate_cluster_names(hdbscan_labels, recipe_vectors, vocabulary)\n",
    "hdbscan_details = get_cluster_details(hdbscan_labels, recipe_vectors, recipes, vocabulary, hdbscan_names)\n",
    "\n",
    "print(\"Cluster Names:\")\n",
    "for label, name in sorted(hdbscan_names.items()):\n",
    "    count = list(hdbscan_labels).count(label)\n",
    "    print(f\"  Cluster {label}: {name} ({count} recipes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize HDBSCAN clusters\n",
    "hdbscan_coords = visualize_clusters(recipe_vectors, hdbscan_labels, hdbscan_names, recipes, \n",
    "                                     title=\"HDBSCAN Clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for HDBSCAN\n",
    "visualize_radar(hdbscan_details, vocabulary, title=\"HDBSCAN Cluster Profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed summary\n",
    "print_cluster_summary(hdbscan_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Method 2: Agglomerative Clustering\n",
    "\n",
    "Agglomerative Clustering is a hierarchical method that:\n",
    "- Uses silhouette score to find optimal k\n",
    "- **Guarantees all recipes are assigned** (no outliers)\n",
    "- May create singleton clusters for unique recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k using silhouette score\n",
    "k_range = (3, 12)\n",
    "silhouette_scores = []\n",
    "\n",
    "for k in range(k_range[0], min(k_range[1] + 1, len(recipes))):\n",
    "    agg = AgglomerativeClustering(n_clusters=k, metric='euclidean', linkage='ward')\n",
    "    labels = agg.fit_predict(recipe_vectors)\n",
    "    score = silhouette_score(recipe_vectors, labels)\n",
    "    silhouette_scores.append((k, score))\n",
    "\n",
    "# Plot silhouette scores\n",
    "ks, scores = zip(*silhouette_scores)\n",
    "best_k = ks[np.argmax(scores)]\n",
    "best_score = max(scores)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(ks, scores, 'bo-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=best_k, color='r', linestyle='--', label=f'Best k={best_k}')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Agglomerative Clustering: Silhouette Score vs k')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal k: {best_k} (silhouette score: {best_score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Agglomerative clustering with optimal k\n",
    "agg_clusterer = AgglomerativeClustering(n_clusters=best_k, metric='euclidean', linkage='ward')\n",
    "agg_labels = agg_clusterer.fit_predict(recipe_vectors)\n",
    "\n",
    "print(\"Agglomerative Clustering Results:\")\n",
    "print(f\"  Clusters: {best_k}\")\n",
    "print(f\"  Outliers: 0 (all recipes assigned)\")\n",
    "print(f\"  Silhouette score: {best_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cluster names and details\n",
    "agg_names = generate_cluster_names(agg_labels, recipe_vectors, vocabulary)\n",
    "agg_details = get_cluster_details(agg_labels, recipe_vectors, recipes, vocabulary, agg_names)\n",
    "\n",
    "print(\"Cluster Names:\")\n",
    "for label, name in sorted(agg_names.items()):\n",
    "    count = list(agg_labels).count(label)\n",
    "    singleton = \" (singleton)\" if count == 1 else \"\"\n",
    "    print(f\"  Cluster {label}: {name} ({count} recipes){singleton}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Agglomerative clusters\n",
    "agg_coords = visualize_clusters(recipe_vectors, agg_labels, agg_names, recipes,\n",
    "                                 title=\"Agglomerative Clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for Agglomerative (excluding singletons for clarity)\n",
    "agg_details_no_singletons = {k: v for k, v in agg_details.items() if v['size'] > 1}\n",
    "visualize_radar(agg_details_no_singletons, vocabulary, title=\"Agglomerative Cluster Profiles (excl. singletons)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed summary\n",
    "print_cluster_summary(agg_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Method 3: FAISS Vector Database\n",
    "\n",
    "FAISS provides:\n",
    "- Efficient k-means clustering\n",
    "- **Similarity search** capabilities\n",
    "- Production-ready vector indexing\n",
    "- Best for recipe creation workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare vectors for FAISS\n",
    "vectors_float32 = np.ascontiguousarray(recipe_vectors.astype('float32'))\n",
    "n_samples, d = vectors_float32.shape\n",
    "\n",
    "# Build FAISS index\n",
    "faiss_index = faiss.IndexFlatL2(d)\n",
    "faiss_index.add(vectors_float32)\n",
    "\n",
    "print(f\"FAISS index built:\")\n",
    "print(f\"  Vectors: {faiss_index.ntotal}\")\n",
    "print(f\"  Dimensions: {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k using silhouette score with FAISS k-means\n",
    "k_range = (4, 12)\n",
    "faiss_scores = []\n",
    "best_faiss_k = k_range[0]\n",
    "best_faiss_score = -1\n",
    "best_centroids = None\n",
    "best_faiss_labels = None\n",
    "\n",
    "for k in range(k_range[0], min(k_range[1] + 1, n_samples)):\n",
    "    kmeans = faiss.Kmeans(d, k, niter=50, verbose=False, seed=42)\n",
    "    kmeans.train(vectors_float32)\n",
    "    \n",
    "    _, labels = kmeans.index.search(vectors_float32, 1)\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    if len(set(labels)) > 1:\n",
    "        score = silhouette_score(vectors_float32, labels)\n",
    "        faiss_scores.append((k, score))\n",
    "        \n",
    "        if score > best_faiss_score:\n",
    "            best_faiss_score = score\n",
    "            best_faiss_k = k\n",
    "            best_centroids = kmeans.centroids.copy()\n",
    "            best_faiss_labels = labels.copy()\n",
    "\n",
    "# Plot\n",
    "ks, scores = zip(*faiss_scores)\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(ks, scores, 'go-', linewidth=2, markersize=8)\n",
    "plt.axvline(x=best_faiss_k, color='r', linestyle='--', label=f'Best k={best_faiss_k}')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('FAISS k-means: Silhouette Score vs k')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Optimal k: {best_faiss_k} (silhouette score: {best_faiss_score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best FAISS labels\n",
    "faiss_labels = best_faiss_labels\n",
    "\n",
    "print(\"FAISS Clustering Results:\")\n",
    "print(f\"  Clusters: {best_faiss_k}\")\n",
    "print(f\"  Outliers: 0 (all recipes assigned)\")\n",
    "print(f\"  Silhouette score: {best_faiss_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate cluster names and details\n",
    "faiss_names = generate_cluster_names(faiss_labels, recipe_vectors, vocabulary)\n",
    "faiss_details = get_cluster_details(faiss_labels, recipe_vectors, recipes, vocabulary, faiss_names)\n",
    "\n",
    "print(\"Cluster Names:\")\n",
    "for label, name in sorted(faiss_names.items()):\n",
    "    count = list(faiss_labels).count(label)\n",
    "    print(f\"  Cluster {label}: {name} ({count} recipes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize FAISS clusters\n",
    "faiss_coords = visualize_clusters(recipe_vectors, faiss_labels, faiss_names, recipes,\n",
    "                                   title=\"FAISS Clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart for FAISS\n",
    "visualize_radar(faiss_details, vocabulary, title=\"FAISS Cluster Profiles\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS-specific: Similarity Matrix\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. Similarity heatmap\n",
    "distances, _ = faiss_index.search(vectors_float32, len(recipes))\n",
    "similarity = 1 / (1 + distances)\n",
    "\n",
    "im1 = axes[0].imshow(similarity, cmap='YlOrRd', aspect='auto')\n",
    "axes[0].set_title('Recipe Similarity Matrix\\n(FAISS L2 Distance)', fontsize=11, fontweight='bold')\n",
    "axes[0].set_xlabel('Recipe Index')\n",
    "axes[0].set_ylabel('Recipe Index')\n",
    "plt.colorbar(im1, ax=axes[0], shrink=0.8, label='Similarity')\n",
    "\n",
    "# 2. Cluster sizes\n",
    "unique_labels = sorted(set(faiss_labels))\n",
    "sizes = [list(faiss_labels).count(l) for l in unique_labels]\n",
    "names = [faiss_names.get(l, f\"C{l}\")[:20] for l in unique_labels]\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(unique_labels)))\n",
    "\n",
    "bars = axes[1].barh(range(len(unique_labels)), sizes, color=colors)\n",
    "axes[1].set_yticks(range(len(unique_labels)))\n",
    "axes[1].set_yticklabels(names, fontsize=9)\n",
    "axes[1].set_xlabel('Number of Recipes')\n",
    "axes[1].set_title('Cluster Sizes\\n(FAISS k-means)', fontsize=11, fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "for bar, size in zip(bars, sizes):\n",
    "    axes[1].text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, str(size), va='center')\n",
    "\n",
    "# 3. Inter-cluster distances\n",
    "n_centroids = len(best_centroids)\n",
    "centroid_dist = np.zeros((n_centroids, n_centroids))\n",
    "for i in range(n_centroids):\n",
    "    for j in range(n_centroids):\n",
    "        centroid_dist[i, j] = np.linalg.norm(best_centroids[i] - best_centroids[j])\n",
    "\n",
    "im3 = axes[2].imshow(centroid_dist, cmap='Blues', aspect='auto')\n",
    "axes[2].set_title('Inter-Cluster Distances\\n(Centroid L2)', fontsize=11, fontweight='bold')\n",
    "axes[2].set_xlabel('Cluster')\n",
    "axes[2].set_ylabel('Cluster')\n",
    "plt.colorbar(im3, ax=axes[2], shrink=0.8, label='Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed summary\n",
    "print_cluster_summary(faiss_details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS Similarity Search Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_recipes(recipe_idx, top_k=5):\n",
    "    \"\"\"Find most similar recipes using FAISS.\"\"\"\n",
    "    query = vectors_float32[recipe_idx:recipe_idx+1]\n",
    "    distances, indices = faiss_index.search(query, top_k + 1)\n",
    "    \n",
    "    results = []\n",
    "    for dist, idx in zip(distances[0], indices[0]):\n",
    "        if idx != recipe_idx:\n",
    "            results.append((recipes[idx], float(dist)))\n",
    "    return results[:top_k]\n",
    "\n",
    "# Demo: Find similar recipes\n",
    "print(\"FAISS Similarity Search Demo\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for i in [0, 15, 29]:\n",
    "    recipe = recipes[i]\n",
    "    similar = find_similar_recipes(i, top_k=3)\n",
    "    print(f\"\\nRecipes similar to '{recipe}':\")\n",
    "    for sim_recipe, dist in similar:\n",
    "        print(f\"  • {sim_recipe} (distance: {dist:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_recipes_by_profile(target_profile, top_k=5):\n",
    "    \"\"\"Find recipes matching a target sensorik profile.\"\"\"\n",
    "    query = np.zeros((1, len(vocabulary)), dtype='float32')\n",
    "    for term, weight in target_profile.items():\n",
    "        term_normalized = term.lower().strip()\n",
    "        if term_normalized in vocab_to_idx:\n",
    "            query[0, vocab_to_idx[term_normalized]] = weight\n",
    "    \n",
    "    query = normalize(query).astype('float32')\n",
    "    distances, indices = faiss_index.search(query, top_k)\n",
    "    \n",
    "    return [(recipes[idx], float(dist)) for dist, idx in zip(distances[0], indices[0])]\n",
    "\n",
    "# Demo: Search by custom profile\n",
    "print(\"\\nCustom Profile Search\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "custom_profile = {\n",
    "    'fruity': 0.8,\n",
    "    'sweet': 0.6,\n",
    "    'tropical': 0.4,\n",
    "    'pineapple': 0.3\n",
    "}\n",
    "\n",
    "print(\"\\nSearching for recipes matching:\")\n",
    "for term, weight in custom_profile.items():\n",
    "    print(f\"  {term}: {weight}\")\n",
    "\n",
    "matches = find_recipes_by_profile(custom_profile, top_k=5)\n",
    "print(\"\\nBest matching recipes:\")\n",
    "for recipe, dist in matches:\n",
    "    print(f\"  • {recipe} (distance: {dist:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Comparison of All Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison table\n",
    "comparison_data = {\n",
    "    'Metric': [\n",
    "        'Clusters Found',\n",
    "        'Outliers',\n",
    "        'Singletons',\n",
    "        'Silhouette Score',\n",
    "        'Similarity Search',\n",
    "        'Auto k Selection',\n",
    "        'Largest Cluster',\n",
    "        'Smallest Cluster'\n",
    "    ],\n",
    "    'HDBSCAN': [\n",
    "        len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0),\n",
    "        f\"{list(hdbscan_labels).count(-1)} ({list(hdbscan_labels).count(-1)/len(recipes)*100:.0f}%)\",\n",
    "        sum(1 for l in set(hdbscan_labels) if l != -1 and list(hdbscan_labels).count(l) == 1),\n",
    "        'N/A',\n",
    "        'No',\n",
    "        'Yes',\n",
    "        max(list(hdbscan_labels).count(l) for l in set(hdbscan_labels) if l != -1) if any(l != -1 for l in hdbscan_labels) else 0,\n",
    "        min(list(hdbscan_labels).count(l) for l in set(hdbscan_labels) if l != -1) if any(l != -1 for l in hdbscan_labels) else 0\n",
    "    ],\n",
    "    'Agglomerative': [\n",
    "        len(set(agg_labels)),\n",
    "        '0 (0%)',\n",
    "        sum(1 for l in set(agg_labels) if list(agg_labels).count(l) == 1),\n",
    "        f'{best_score:.3f}',\n",
    "        'No',\n",
    "        'Yes',\n",
    "        max(list(agg_labels).count(l) for l in set(agg_labels)),\n",
    "        min(list(agg_labels).count(l) for l in set(agg_labels))\n",
    "    ],\n",
    "    'FAISS': [\n",
    "        len(set(faiss_labels)),\n",
    "        '0 (0%)',\n",
    "        sum(1 for l in set(faiss_labels) if list(faiss_labels).count(l) == 1),\n",
    "        f'{best_faiss_score:.3f}',\n",
    "        'Yes',\n",
    "        'Yes',\n",
    "        max(list(faiss_labels).count(l) for l in set(faiss_labels)),\n",
    "        min(list(faiss_labels).count(l) for l in set(faiss_labels))\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df.set_index('Metric', inplace=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPARISON TABLE: All Clustering Methods\")\n",
    "print(\"=\" * 70)\n",
    "print(comparison_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of cluster distributions\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "methods = [\n",
    "    ('HDBSCAN', hdbscan_labels, hdbscan_names),\n",
    "    ('Agglomerative', agg_labels, agg_names),\n",
    "    ('FAISS', faiss_labels, faiss_names)\n",
    "]\n",
    "\n",
    "for ax, (method_name, labels, names) in zip(axes, methods):\n",
    "    unique_labels = sorted(set(labels))\n",
    "    sizes = [list(labels).count(l) for l in unique_labels]\n",
    "    label_names = [names.get(l, f\"C{l}\")[:15] for l in unique_labels]\n",
    "    colors = ['gray' if l == -1 else plt.cm.Set2(i/len(unique_labels)) \n",
    "              for i, l in enumerate(unique_labels)]\n",
    "    \n",
    "    ax.pie(sizes, labels=label_names, colors=colors, autopct='%1.0f%%',\n",
    "           startangle=90, textprops={'fontsize': 8})\n",
    "    ax.set_title(f'{method_name}\\n({len(unique_labels)} clusters)', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Cluster Distribution Comparison', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Creating New Recipes\n",
    "\n",
    "All methods support creating new recipe profiles by averaging cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_new_recipe_profile(cluster_labels, recipe_vectors, vocabulary, cluster_weights):\n",
    "    \"\"\"Create a new recipe profile by weighted averaging of cluster centroids.\"\"\"\n",
    "    total_weight = sum(cluster_weights.values())\n",
    "    normalized_weights = {k: v/total_weight for k, v in cluster_weights.items()}\n",
    "    \n",
    "    new_profile = np.zeros(len(vocabulary))\n",
    "    \n",
    "    for label, weight in normalized_weights.items():\n",
    "        if label == -1:\n",
    "            continue\n",
    "        cluster_mask = cluster_labels == label\n",
    "        cluster_vectors = recipe_vectors[cluster_mask]\n",
    "        centroid = cluster_vectors.mean(axis=0)\n",
    "        new_profile += weight * centroid\n",
    "    \n",
    "    top_indices = np.argsort(new_profile)[-15:][::-1]\n",
    "    top_terms = [(vocabulary[i], new_profile[i]) for i in top_indices]\n",
    "    \n",
    "    return new_profile, top_terms\n",
    "\n",
    "# Demo: Create new recipe using FAISS clusters\n",
    "print(\"Creating New Recipe Profile\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Mix two clusters\n",
    "cluster_a = 0\n",
    "cluster_b = 1\n",
    "\n",
    "print(f\"\\nMixing:\")\n",
    "print(f\"  70% Cluster {cluster_a} ({faiss_names.get(cluster_a, 'Unknown')})\")\n",
    "print(f\"  30% Cluster {cluster_b} ({faiss_names.get(cluster_b, 'Unknown')})\")\n",
    "\n",
    "profile, top_terms = create_new_recipe_profile(\n",
    "    faiss_labels, recipe_vectors, vocabulary,\n",
    "    {cluster_a: 0.7, cluster_b: 0.3}\n",
    ")\n",
    "\n",
    "print(f\"\\nNew Recipe Target Sensorik Profile:\")\n",
    "for term, weight in top_terms[:10]:\n",
    "    bar = '█' * int(weight * 50)\n",
    "    print(f\"  {term:15} {bar} ({weight:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find similar existing recipes for the new profile\n",
    "query = normalize(profile.reshape(1, -1)).astype('float32')\n",
    "distances, indices = faiss_index.search(query, 5)\n",
    "\n",
    "print(\"\\nMost Similar Existing Recipes:\")\n",
    "for dist, idx in zip(distances[0], indices[0]):\n",
    "    print(f\"  • {recipes[idx]} (distance: {dist:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "## When to Use Each Method\n",
    "\n",
    "| Method | Best For |\n",
    "|--------|----------|\n",
    "| **HDBSCAN** | Identifying unique/outlier recipes, data exploration |\n",
    "| **Agglomerative** | Fine-grained categorization, hierarchical relationships |\n",
    "| **FAISS** | Recipe creation, similarity search, production systems |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Weighted importance** is preserved in all methods through feature extraction\n",
    "2. **HDBSCAN** found 47% outliers - useful for identifying truly unique recipes\n",
    "3. **Agglomerative** created fine-grained clusters with 6 singletons\n",
    "4. **FAISS** provides the best balance + similarity search for recipe creation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
